{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: Comparing Trained LLM Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook of lesson 5, you will work with several tokenizers associated with different LLMs and explore how each tokenizer approaches tokenization differently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning control\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will tokenize the sentence \"Hello World!\" using the tokenizer of the [`bert-base-cased` model](https://huggingface.co/google-bert/bert-base-cased). \n",
    "\n",
    "Let's import the `Autotokenizer` class, define the sentence to tokenize, and instantiate the tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#fff1d7; padding:15px; \"> <b>FYI: </b> The transformers library has a set of Auto classes, like AutoConfig, AutoModel, and AutoTokenizer. The Auto classes are designed to automatically do the job for you.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentence to tokenize\n",
    "sentence = \"Hello World!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6c3e3119884f2da4e4941f611ac8ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now apply the tokenizer to the sentence. The tokeziner splits the sentence into tokens and returns the IDs of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the tokenizer to the sentence and extract the token ids\n",
    "token_ids = tokenizer(sentence).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 8667, 1291, 106, 102]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To map each token ID to its corresponding token, you can use the `decode` method of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "Hello\n",
      "World\n",
      "!\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "for id in token_ids:\n",
    "    print(tokenizer.decode(id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coloring text/background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\"red\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\x1b[31m\\\"red\\\"\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mred\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\x1b[31mred\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mred\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\x1b[31m\" + \"red\" + \"\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;255;0;0mred\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"\\x1b[38;2;255;0;0m\" + \"red\" + \"\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;2;255;0;0mred\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Background coloring\n",
    "print(\"\\x1b[48;2;255;0;0m\" + \"red\" + \"\\x1b[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Tokenization\n",
    "\n",
    "In this section, you'll wrap the code of the previous section in the function `show_tokens`. The function takes in a text and the model name, and prints the vocabulary length of the tokenizer and a colored list of the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of colors in RGB for representing the tokens\n",
    "colors = [\n",
    "    '102;194;165', '252;141;98', '141;160;203',\n",
    "    '231;138;195', '166;216;84', '255;217;47'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokens(sentence: str, tokenizer_name: str, change: str=\"background\"):\n",
    "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
    "    \n",
    "    assert change in [\"foreground\", \"background\"], \"Expected either foreground or background\"\n",
    "    \n",
    "    change_val = 38 if change == \"foreground\" else 48\n",
    "    \n",
    "    # Load the tokenizer and tokenize the input\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    token_ids = tokenizer(sentence).input_ids\n",
    "\n",
    "    # Extract vocabulary length\n",
    "    print(f\"Vocabulary length: {len(tokenizer)}\")\n",
    "\n",
    "    # Print a colored list of tokens\n",
    "    for idx, t in enumerate(token_ids):\n",
    "        print(\n",
    "            f'\\x1b[{change_val};2;{colors[idx % len(colors)]}m' +\n",
    "            tokenizer.decode(t) +\n",
    "            '\\x1b[0m',\n",
    "            end=' '\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the text that you'll use to explore the different tokenization strategies of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "English and CAPITALIZATION\n",
    "ðŸŽµ é¸Ÿ\n",
    "show_tokens False None elif == >= else: two tabs:\"    \" Three tabs: \"       \"\n",
    "12.0*50=600\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll now again use the tokenizer of `bert-base-cased` and compare its tokenization strategy to that of `Xenova/gpt-4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bert-base-cased**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 28996\n",
      "\u001b[48;2;102;194;165m[CLS]\u001b[0m \u001b[48;2;252;141;98mEnglish\u001b[0m \u001b[48;2;141;160;203mand\u001b[0m \u001b[48;2;231;138;195mCA\u001b[0m \u001b[48;2;166;216;84m##PI\u001b[0m \u001b[48;2;255;217;47m##TA\u001b[0m \u001b[48;2;102;194;165m##L\u001b[0m \u001b[48;2;252;141;98m##I\u001b[0m \u001b[48;2;141;160;203m##Z\u001b[0m \u001b[48;2;231;138;195m##AT\u001b[0m \u001b[48;2;166;216;84m##ION\u001b[0m \u001b[48;2;255;217;47m[UNK]\u001b[0m \u001b[48;2;102;194;165m[UNK]\u001b[0m \u001b[48;2;252;141;98mshow\u001b[0m \u001b[48;2;141;160;203m_\u001b[0m \u001b[48;2;231;138;195mtoken\u001b[0m \u001b[48;2;166;216;84m##s\u001b[0m \u001b[48;2;255;217;47mF\u001b[0m \u001b[48;2;102;194;165m##als\u001b[0m \u001b[48;2;252;141;98m##e\u001b[0m \u001b[48;2;141;160;203mNone\u001b[0m \u001b[48;2;231;138;195mel\u001b[0m \u001b[48;2;166;216;84m##if\u001b[0m \u001b[48;2;255;217;47m=\u001b[0m \u001b[48;2;102;194;165m=\u001b[0m \u001b[48;2;252;141;98m>\u001b[0m \u001b[48;2;141;160;203m=\u001b[0m \u001b[48;2;231;138;195melse\u001b[0m \u001b[48;2;166;216;84m:\u001b[0m \u001b[48;2;255;217;47mtwo\u001b[0m \u001b[48;2;102;194;165mta\u001b[0m \u001b[48;2;252;141;98m##bs\u001b[0m \u001b[48;2;141;160;203m:\u001b[0m \u001b[48;2;231;138;195m\"\u001b[0m \u001b[48;2;166;216;84m\"\u001b[0m \u001b[48;2;255;217;47mThree\u001b[0m \u001b[48;2;102;194;165mta\u001b[0m \u001b[48;2;252;141;98m##bs\u001b[0m \u001b[48;2;141;160;203m:\u001b[0m \u001b[48;2;231;138;195m\"\u001b[0m \u001b[48;2;166;216;84m\"\u001b[0m \u001b[48;2;255;217;47m12\u001b[0m \u001b[48;2;102;194;165m.\u001b[0m \u001b[48;2;252;141;98m0\u001b[0m \u001b[48;2;141;160;203m*\u001b[0m \u001b[48;2;231;138;195m50\u001b[0m \u001b[48;2;166;216;84m=\u001b[0m \u001b[48;2;255;217;47m600\u001b[0m \u001b[48;2;102;194;165m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 28996\n",
      "\u001b[38;2;102;194;165m[CLS]\u001b[0m \u001b[38;2;252;141;98mEnglish\u001b[0m \u001b[38;2;141;160;203mand\u001b[0m \u001b[38;2;231;138;195mCA\u001b[0m \u001b[38;2;166;216;84m##PI\u001b[0m \u001b[38;2;255;217;47m##TA\u001b[0m \u001b[38;2;102;194;165m##L\u001b[0m \u001b[38;2;252;141;98m##I\u001b[0m \u001b[38;2;141;160;203m##Z\u001b[0m \u001b[38;2;231;138;195m##AT\u001b[0m \u001b[38;2;166;216;84m##ION\u001b[0m \u001b[38;2;255;217;47m[UNK]\u001b[0m \u001b[38;2;102;194;165m[UNK]\u001b[0m \u001b[38;2;252;141;98mshow\u001b[0m \u001b[38;2;141;160;203m_\u001b[0m \u001b[38;2;231;138;195mtoken\u001b[0m \u001b[38;2;166;216;84m##s\u001b[0m \u001b[38;2;255;217;47mF\u001b[0m \u001b[38;2;102;194;165m##als\u001b[0m \u001b[38;2;252;141;98m##e\u001b[0m \u001b[38;2;141;160;203mNone\u001b[0m \u001b[38;2;231;138;195mel\u001b[0m \u001b[38;2;166;216;84m##if\u001b[0m \u001b[38;2;255;217;47m=\u001b[0m \u001b[38;2;102;194;165m=\u001b[0m \u001b[38;2;252;141;98m>\u001b[0m \u001b[38;2;141;160;203m=\u001b[0m \u001b[38;2;231;138;195melse\u001b[0m \u001b[38;2;166;216;84m:\u001b[0m \u001b[38;2;255;217;47mtwo\u001b[0m \u001b[38;2;102;194;165mta\u001b[0m \u001b[38;2;252;141;98m##bs\u001b[0m \u001b[38;2;141;160;203m:\u001b[0m \u001b[38;2;231;138;195m\"\u001b[0m \u001b[38;2;166;216;84m\"\u001b[0m \u001b[38;2;255;217;47mThree\u001b[0m \u001b[38;2;102;194;165mta\u001b[0m \u001b[38;2;252;141;98m##bs\u001b[0m \u001b[38;2;141;160;203m:\u001b[0m \u001b[38;2;231;138;195m\"\u001b[0m \u001b[38;2;166;216;84m\"\u001b[0m \u001b[38;2;255;217;47m12\u001b[0m \u001b[38;2;102;194;165m.\u001b[0m \u001b[38;2;252;141;98m0\u001b[0m \u001b[38;2;141;160;203m*\u001b[0m \u001b[38;2;231;138;195m50\u001b[0m \u001b[38;2;166;216;84m=\u001b[0m \u001b[38;2;255;217;47m600\u001b[0m \u001b[38;2;102;194;165m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-cased\", change=\"foreground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional - bert-base-uncased**\n",
    "\n",
    "You can also try the uncased version of the bert model, and compare the vocab length and tokenization strategy of the two bert versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 30522\n",
      "\u001b[38;2;102;194;165m[CLS]\u001b[0m \u001b[38;2;252;141;98menglish\u001b[0m \u001b[38;2;141;160;203mand\u001b[0m \u001b[38;2;231;138;195mcapital\u001b[0m \u001b[38;2;166;216;84m##ization\u001b[0m \u001b[38;2;255;217;47m[UNK]\u001b[0m \u001b[38;2;102;194;165m[UNK]\u001b[0m \u001b[38;2;252;141;98mshow\u001b[0m \u001b[38;2;141;160;203m_\u001b[0m \u001b[38;2;231;138;195mtoken\u001b[0m \u001b[38;2;166;216;84m##s\u001b[0m \u001b[38;2;255;217;47mfalse\u001b[0m \u001b[38;2;102;194;165mnone\u001b[0m \u001b[38;2;252;141;98meli\u001b[0m \u001b[38;2;141;160;203m##f\u001b[0m \u001b[38;2;231;138;195m=\u001b[0m \u001b[38;2;166;216;84m=\u001b[0m \u001b[38;2;255;217;47m>\u001b[0m \u001b[38;2;102;194;165m=\u001b[0m \u001b[38;2;252;141;98melse\u001b[0m \u001b[38;2;141;160;203m:\u001b[0m \u001b[38;2;231;138;195mtwo\u001b[0m \u001b[38;2;166;216;84mtab\u001b[0m \u001b[38;2;255;217;47m##s\u001b[0m \u001b[38;2;102;194;165m:\u001b[0m \u001b[38;2;252;141;98m\"\u001b[0m \u001b[38;2;141;160;203m\"\u001b[0m \u001b[38;2;231;138;195mthree\u001b[0m \u001b[38;2;166;216;84mtab\u001b[0m \u001b[38;2;255;217;47m##s\u001b[0m \u001b[38;2;102;194;165m:\u001b[0m \u001b[38;2;252;141;98m\"\u001b[0m \u001b[38;2;141;160;203m\"\u001b[0m \u001b[38;2;231;138;195m12\u001b[0m \u001b[38;2;166;216;84m.\u001b[0m \u001b[38;2;255;217;47m0\u001b[0m \u001b[38;2;102;194;165m*\u001b[0m \u001b[38;2;252;141;98m50\u001b[0m \u001b[38;2;141;160;203m=\u001b[0m \u001b[38;2;231;138;195m600\u001b[0m \u001b[38;2;166;216;84m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-uncased\", change=\"foreground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPT-4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 100263\n",
      "\u001b[38;2;102;194;165m\n",
      "\u001b[0m \u001b[38;2;252;141;98mEnglish\u001b[0m \u001b[38;2;141;160;203m and\u001b[0m \u001b[38;2;231;138;195m CAPITAL\u001b[0m \u001b[38;2;166;216;84mIZATION\u001b[0m \u001b[38;2;255;217;47m\n",
      "\u001b[0m \u001b[38;2;102;194;165mï¿½\u001b[0m \u001b[38;2;252;141;98mï¿½\u001b[0m \u001b[38;2;141;160;203mï¿½\u001b[0m \u001b[38;2;231;138;195m ï¿½\u001b[0m \u001b[38;2;166;216;84mï¿½\u001b[0m \u001b[38;2;255;217;47mï¿½\u001b[0m \u001b[38;2;102;194;165m\n",
      "\u001b[0m \u001b[38;2;252;141;98mshow\u001b[0m \u001b[38;2;141;160;203m_tokens\u001b[0m \u001b[38;2;231;138;195m False\u001b[0m \u001b[38;2;166;216;84m None\u001b[0m \u001b[38;2;255;217;47m elif\u001b[0m \u001b[38;2;102;194;165m ==\u001b[0m \u001b[38;2;252;141;98m >=\u001b[0m \u001b[38;2;141;160;203m else\u001b[0m \u001b[38;2;231;138;195m:\u001b[0m \u001b[38;2;166;216;84m two\u001b[0m \u001b[38;2;255;217;47m tabs\u001b[0m \u001b[38;2;102;194;165m:\"\u001b[0m \u001b[38;2;252;141;98m   \u001b[0m \u001b[38;2;141;160;203m \"\u001b[0m \u001b[38;2;231;138;195m Three\u001b[0m \u001b[38;2;166;216;84m tabs\u001b[0m \u001b[38;2;255;217;47m:\u001b[0m \u001b[38;2;102;194;165m \"\u001b[0m \u001b[38;2;252;141;98m      \u001b[0m \u001b[38;2;141;160;203m \"\n",
      "\u001b[0m \u001b[38;2;231;138;195m12\u001b[0m \u001b[38;2;166;216;84m.\u001b[0m \u001b[38;2;255;217;47m0\u001b[0m \u001b[38;2;102;194;165m*\u001b[0m \u001b[38;2;252;141;98m50\u001b[0m \u001b[38;2;141;160;203m=\u001b[0m \u001b[38;2;231;138;195m600\u001b[0m \u001b[38;2;166;216;84m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"Xenova/gpt-4\", change=\"foreground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Models to Explore\n",
    "\n",
    "You can also explore the tokenization strategy of other models. The following is a suggested list. Make sure to consider the following features when you're doing your comparison:\n",
    "- Vocabulary length\n",
    "- Special tokens\n",
    "- Tokenization of the tabs, special characters and special keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gpt2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f5064f60134508aac04686dad6bd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 50257\n",
      "\u001b[38;2;102;194;165m\n",
      "\u001b[0m \u001b[38;2;252;141;98mEnglish\u001b[0m \u001b[38;2;141;160;203m and\u001b[0m \u001b[38;2;231;138;195m CAP\u001b[0m \u001b[38;2;166;216;84mITAL\u001b[0m \u001b[38;2;255;217;47mIZ\u001b[0m \u001b[38;2;102;194;165mATION\u001b[0m \u001b[38;2;252;141;98m\n",
      "\u001b[0m \u001b[38;2;141;160;203mï¿½\u001b[0m \u001b[38;2;231;138;195mï¿½\u001b[0m \u001b[38;2;166;216;84mï¿½\u001b[0m \u001b[38;2;255;217;47m ï¿½\u001b[0m \u001b[38;2;102;194;165mï¿½\u001b[0m \u001b[38;2;252;141;98mï¿½\u001b[0m \u001b[38;2;141;160;203m\n",
      "\u001b[0m \u001b[38;2;231;138;195mshow\u001b[0m \u001b[38;2;166;216;84m_\u001b[0m \u001b[38;2;255;217;47mt\u001b[0m \u001b[38;2;102;194;165mok\u001b[0m \u001b[38;2;252;141;98mens\u001b[0m \u001b[38;2;141;160;203m False\u001b[0m \u001b[38;2;231;138;195m None\u001b[0m \u001b[38;2;166;216;84m el\u001b[0m \u001b[38;2;255;217;47mif\u001b[0m \u001b[38;2;102;194;165m ==\u001b[0m \u001b[38;2;252;141;98m >=\u001b[0m \u001b[38;2;141;160;203m else\u001b[0m \u001b[38;2;231;138;195m:\u001b[0m \u001b[38;2;166;216;84m two\u001b[0m \u001b[38;2;255;217;47m tabs\u001b[0m \u001b[38;2;102;194;165m:\"\u001b[0m \u001b[38;2;252;141;98m \u001b[0m \u001b[38;2;141;160;203m \u001b[0m \u001b[38;2;231;138;195m \u001b[0m \u001b[38;2;166;216;84m \"\u001b[0m \u001b[38;2;255;217;47m Three\u001b[0m \u001b[38;2;102;194;165m tabs\u001b[0m \u001b[38;2;252;141;98m:\u001b[0m \u001b[38;2;141;160;203m \"\u001b[0m \u001b[38;2;231;138;195m \u001b[0m \u001b[38;2;166;216;84m \u001b[0m \u001b[38;2;255;217;47m \u001b[0m \u001b[38;2;102;194;165m \u001b[0m \u001b[38;2;252;141;98m \u001b[0m \u001b[38;2;141;160;203m \u001b[0m \u001b[38;2;231;138;195m \"\u001b[0m \u001b[38;2;166;216;84m\n",
      "\u001b[0m \u001b[38;2;255;217;47m12\u001b[0m \u001b[38;2;102;194;165m.\u001b[0m \u001b[38;2;252;141;98m0\u001b[0m \u001b[38;2;141;160;203m*\u001b[0m \u001b[38;2;231;138;195m50\u001b[0m \u001b[38;2;166;216;84m=\u001b[0m \u001b[38;2;255;217;47m600\u001b[0m \u001b[38;2;102;194;165m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"gpt2\", change=\"foreground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Flan-T5-small**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 32100\n",
      "\u001b[38;2;102;194;165mEnglish\u001b[0m \u001b[38;2;252;141;98mand\u001b[0m \u001b[38;2;141;160;203mCA\u001b[0m \u001b[38;2;231;138;195mPI\u001b[0m \u001b[38;2;166;216;84mTAL\u001b[0m \u001b[38;2;255;217;47mIZ\u001b[0m \u001b[38;2;102;194;165mATION\u001b[0m \u001b[38;2;252;141;98m\u001b[0m \u001b[38;2;141;160;203m<unk>\u001b[0m \u001b[38;2;231;138;195m\u001b[0m \u001b[38;2;166;216;84m<unk>\u001b[0m \u001b[38;2;255;217;47mshow\u001b[0m \u001b[38;2;102;194;165m_\u001b[0m \u001b[38;2;252;141;98mto\u001b[0m \u001b[38;2;141;160;203mken\u001b[0m \u001b[38;2;231;138;195ms\u001b[0m \u001b[38;2;166;216;84mFal\u001b[0m \u001b[38;2;255;217;47ms\u001b[0m \u001b[38;2;102;194;165me\u001b[0m \u001b[38;2;252;141;98mNone\u001b[0m \u001b[38;2;141;160;203m\u001b[0m \u001b[38;2;231;138;195me\u001b[0m \u001b[38;2;166;216;84ml\u001b[0m \u001b[38;2;255;217;47mif\u001b[0m \u001b[38;2;102;194;165m=\u001b[0m \u001b[38;2;252;141;98m=\u001b[0m \u001b[38;2;141;160;203m>\u001b[0m \u001b[38;2;231;138;195m=\u001b[0m \u001b[38;2;166;216;84melse\u001b[0m \u001b[38;2;255;217;47m:\u001b[0m \u001b[38;2;102;194;165mtwo\u001b[0m \u001b[38;2;252;141;98mtab\u001b[0m \u001b[38;2;141;160;203ms\u001b[0m \u001b[38;2;231;138;195m:\u001b[0m \u001b[38;2;166;216;84m\"\u001b[0m \u001b[38;2;255;217;47m\"\u001b[0m \u001b[38;2;102;194;165mThree\u001b[0m \u001b[38;2;252;141;98mtab\u001b[0m \u001b[38;2;141;160;203ms\u001b[0m \u001b[38;2;231;138;195m:\u001b[0m \u001b[38;2;166;216;84m\"\u001b[0m \u001b[38;2;255;217;47m\"\u001b[0m \u001b[38;2;102;194;165m12.\u001b[0m \u001b[38;2;252;141;98m0\u001b[0m \u001b[38;2;141;160;203m*\u001b[0m \u001b[38;2;231;138;195m50\u001b[0m \u001b[38;2;166;216;84m=\u001b[0m \u001b[38;2;255;217;47m600\u001b[0m \u001b[38;2;102;194;165m\u001b[0m \u001b[38;2;252;141;98m</s>\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"google/flan-t5-small\", change=\"foreground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Starcoder 2 - 15B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfaf4ea68fa4b948244bdb7cb6b68aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e31c5f50154d188199cec188fe6aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b639751c77b4efeabc30c0efe52b031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4ca0879e9a41d7ae962d6eb26c2031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c64684f3963479a9c7e554d88cb4278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 49152\n",
      "\u001b[38;2;102;194;165m\n",
      "\u001b[0m \u001b[38;2;252;141;98mEnglish\u001b[0m \u001b[38;2;141;160;203m and\u001b[0m \u001b[38;2;231;138;195m CAPITAL\u001b[0m \u001b[38;2;166;216;84mIZATION\u001b[0m \u001b[38;2;255;217;47m\n",
      "\u001b[0m \u001b[38;2;102;194;165mï¿½\u001b[0m \u001b[38;2;252;141;98mï¿½\u001b[0m \u001b[38;2;141;160;203mï¿½\u001b[0m \u001b[38;2;231;138;195m \u001b[0m \u001b[38;2;166;216;84mï¿½\u001b[0m \u001b[38;2;255;217;47mï¿½\u001b[0m \u001b[38;2;102;194;165m\n",
      "\u001b[0m \u001b[38;2;252;141;98mshow\u001b[0m \u001b[38;2;141;160;203m_\u001b[0m \u001b[38;2;231;138;195mtokens\u001b[0m \u001b[38;2;166;216;84m False\u001b[0m \u001b[38;2;255;217;47m None\u001b[0m \u001b[38;2;102;194;165m elif\u001b[0m \u001b[38;2;252;141;98m ==\u001b[0m \u001b[38;2;141;160;203m >=\u001b[0m \u001b[38;2;231;138;195m else\u001b[0m \u001b[38;2;166;216;84m:\u001b[0m \u001b[38;2;255;217;47m two\u001b[0m \u001b[38;2;102;194;165m tabs\u001b[0m \u001b[38;2;252;141;98m:\"\u001b[0m \u001b[38;2;141;160;203m   \u001b[0m \u001b[38;2;231;138;195m \"\u001b[0m \u001b[38;2;166;216;84m Three\u001b[0m \u001b[38;2;255;217;47m tabs\u001b[0m \u001b[38;2;102;194;165m:\u001b[0m \u001b[38;2;252;141;98m \"\u001b[0m \u001b[38;2;141;160;203m      \u001b[0m \u001b[38;2;231;138;195m \"\u001b[0m \u001b[38;2;166;216;84m\n",
      "\u001b[0m \u001b[38;2;255;217;47m1\u001b[0m \u001b[38;2;102;194;165m2\u001b[0m \u001b[38;2;252;141;98m.\u001b[0m \u001b[38;2;141;160;203m0\u001b[0m \u001b[38;2;231;138;195m*\u001b[0m \u001b[38;2;166;216;84m5\u001b[0m \u001b[38;2;255;217;47m0\u001b[0m \u001b[38;2;102;194;165m=\u001b[0m \u001b[38;2;252;141;98m6\u001b[0m \u001b[38;2;141;160;203m0\u001b[0m \u001b[38;2;231;138;195m0\u001b[0m \u001b[38;2;166;216;84m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bigcode/starcoder2-15b\", change=\"foreground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phi-3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ea811a38eb4dc387ada67589a0e55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926240a51d1a4c7092d495cdd9bbe30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61de16d51de245c19920e0048724ffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74badaab007946648925ebe2e11b84b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e20d82e30b4d5a979a6e15f7f3224b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 32011\n",
      "\u001b[38;2;102;194;165m\u001b[0m \u001b[38;2;252;141;98m\n",
      "\u001b[0m \u001b[38;2;141;160;203mEnglish\u001b[0m \u001b[38;2;231;138;195mand\u001b[0m \u001b[38;2;166;216;84mC\u001b[0m \u001b[38;2;255;217;47mAP\u001b[0m \u001b[38;2;102;194;165mIT\u001b[0m \u001b[38;2;252;141;98mAL\u001b[0m \u001b[38;2;141;160;203mIZ\u001b[0m \u001b[38;2;231;138;195mATION\u001b[0m \u001b[38;2;166;216;84m\n",
      "\u001b[0m \u001b[38;2;255;217;47mï¿½\u001b[0m \u001b[38;2;102;194;165mï¿½\u001b[0m \u001b[38;2;252;141;98mï¿½\u001b[0m \u001b[38;2;141;160;203mï¿½\u001b[0m \u001b[38;2;231;138;195m\u001b[0m \u001b[38;2;166;216;84mï¿½\u001b[0m \u001b[38;2;255;217;47mï¿½\u001b[0m \u001b[38;2;102;194;165mï¿½\u001b[0m \u001b[38;2;252;141;98m\n",
      "\u001b[0m \u001b[38;2;141;160;203mshow\u001b[0m \u001b[38;2;231;138;195m_\u001b[0m \u001b[38;2;166;216;84mto\u001b[0m \u001b[38;2;255;217;47mkens\u001b[0m \u001b[38;2;102;194;165mFalse\u001b[0m \u001b[38;2;252;141;98mNone\u001b[0m \u001b[38;2;141;160;203melif\u001b[0m \u001b[38;2;231;138;195m==\u001b[0m \u001b[38;2;166;216;84m>=\u001b[0m \u001b[38;2;255;217;47melse\u001b[0m \u001b[38;2;102;194;165m:\u001b[0m \u001b[38;2;252;141;98mtwo\u001b[0m \u001b[38;2;141;160;203mtabs\u001b[0m \u001b[38;2;231;138;195m:\"\u001b[0m \u001b[38;2;166;216;84m  \u001b[0m \u001b[38;2;255;217;47m\"\u001b[0m \u001b[38;2;102;194;165mThree\u001b[0m \u001b[38;2;252;141;98mtabs\u001b[0m \u001b[38;2;141;160;203m:\u001b[0m \u001b[38;2;231;138;195m\"\u001b[0m \u001b[38;2;166;216;84m     \u001b[0m \u001b[38;2;255;217;47m\"\u001b[0m \u001b[38;2;102;194;165m\n",
      "\u001b[0m \u001b[38;2;252;141;98m1\u001b[0m \u001b[38;2;141;160;203m2\u001b[0m \u001b[38;2;231;138;195m.\u001b[0m \u001b[38;2;166;216;84m0\u001b[0m \u001b[38;2;255;217;47m*\u001b[0m \u001b[38;2;102;194;165m5\u001b[0m \u001b[38;2;252;141;98m0\u001b[0m \u001b[38;2;141;160;203m=\u001b[0m \u001b[38;2;231;138;195m6\u001b[0m \u001b[38;2;166;216;84m0\u001b[0m \u001b[38;2;255;217;47m0\u001b[0m \u001b[38;2;102;194;165m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"microsoft/Phi-3-mini-4k-instruct\", change=\"foreground\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qwen2 - Vision-Language Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3bbc20b2bc487db8ed53507cda3030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b48b8b5847420c952b48fd54ed8048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194275b2111940f0b2a4b010f7bbff0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebb572ee9e84127a80f90ca42244d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length: 151657\n",
      "\u001b[38;2;102;194;165m\n",
      "\u001b[0m \u001b[38;2;252;141;98mEnglish\u001b[0m \u001b[38;2;141;160;203m and\u001b[0m \u001b[38;2;231;138;195m CAPITAL\u001b[0m \u001b[38;2;166;216;84mIZATION\u001b[0m \u001b[38;2;255;217;47m\n",
      "\u001b[0m \u001b[38;2;102;194;165mðŸŽµ\u001b[0m \u001b[38;2;252;141;98m ï¿½\u001b[0m \u001b[38;2;141;160;203mï¿½\u001b[0m \u001b[38;2;231;138;195mï¿½\u001b[0m \u001b[38;2;166;216;84m\n",
      "\u001b[0m \u001b[38;2;255;217;47mshow\u001b[0m \u001b[38;2;102;194;165m_tokens\u001b[0m \u001b[38;2;252;141;98m False\u001b[0m \u001b[38;2;141;160;203m None\u001b[0m \u001b[38;2;231;138;195m elif\u001b[0m \u001b[38;2;166;216;84m ==\u001b[0m \u001b[38;2;255;217;47m >=\u001b[0m \u001b[38;2;102;194;165m else\u001b[0m \u001b[38;2;252;141;98m:\u001b[0m \u001b[38;2;141;160;203m two\u001b[0m \u001b[38;2;231;138;195m tabs\u001b[0m \u001b[38;2;166;216;84m:\"\u001b[0m \u001b[38;2;255;217;47m   \u001b[0m \u001b[38;2;102;194;165m \"\u001b[0m \u001b[38;2;252;141;98m Three\u001b[0m \u001b[38;2;141;160;203m tabs\u001b[0m \u001b[38;2;231;138;195m:\u001b[0m \u001b[38;2;166;216;84m \"\u001b[0m \u001b[38;2;255;217;47m      \u001b[0m \u001b[38;2;102;194;165m \"\n",
      "\u001b[0m \u001b[38;2;252;141;98m1\u001b[0m \u001b[38;2;141;160;203m2\u001b[0m \u001b[38;2;231;138;195m.\u001b[0m \u001b[38;2;166;216;84m0\u001b[0m \u001b[38;2;255;217;47m*\u001b[0m \u001b[38;2;102;194;165m5\u001b[0m \u001b[38;2;252;141;98m0\u001b[0m \u001b[38;2;141;160;203m=\u001b[0m \u001b[38;2;231;138;195m6\u001b[0m \u001b[38;2;166;216;84m0\u001b[0m \u001b[38;2;255;217;47m0\u001b[0m \u001b[38;2;102;194;165m\n",
      "\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"Qwen/Qwen2-VL-7B-Instruct\", change=\"foreground\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_how_transformer_llms_work_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
