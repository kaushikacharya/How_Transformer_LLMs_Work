# How Transformer LLMs Work

## About

This repository contains

- [Course notes](#course-contents)
- [Assignments](#assignments)

## Course Information

- Instructor: Sebastian Witalec (Head of Developer Relations at Weaviate)
- [Course Website](https://www.deeplearning.ai/short-courses/how-transformer-llms-work/)
- Additional resources (collected for better understanding of the concepts)

## Course Contents

|#|     Lesson  |   Description   |
|-|-------------|-----------------|
|0|[Introduction](./notes/Lesson_0.md)|<ul><li>Transformer outline</li><li>Course outline</li></ul>|
|1|[Understanding Language Models: Language as a Bag-of-Words](./notes/Lesson_1.md)|<ul><li>Evolution of numerical representation of language</li><li>Example of Bag-of-Words' vector representation</li></ul>|
|2|[Understanding Language Models: (Word) Embeddings](./notes/Lesson_2.md)|<ul><li>Drawbacks of Bag-of-Words</li><li>Word2Vec explanation</li></ul>|
|3|[Understanding Language Models: Encoding and Decoding Context with Attention](./notes/Lesson_3.md)|<ul><li>Drawbacks of Word2Vec embeddings</li><li>Recurrent Neural Networks (RNNs)</li><li>Attention Mechanism</li></ul>|
|4|[Understanding Language Models: Transformers](./notes/Lesson_4.md)|<ul><li>Transformer architecture</li><li>Representation vs Generative Models</li><li>Context Length</li><li>Evolution of Generative AI over recent years</li></ul>|
|5|[Tokenizers](./notes/Lesson_5.md)|<ul><li>Contextualized embeddings</li><li>Tokenization</li></ul>|

## Assignments

  |Lesson|         Assignment        |   Description   |
  |-------|---------------------------|-----------------|
  |#5|[Comparing Trained LLM Tokenizers](./notes/Lesson_5.md#notebook)|<ul><li>Explore tokenization by different tokenizers</li><li>Visualize tokens in different colors using ANSI escape codes </li></ul>|
